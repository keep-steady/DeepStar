{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAC: The StarCraft Multi-Agent Challenge\n",
    "\n",
    "- MARL (Multi-agent reinforcement learning) \n",
    "- 주요 과제들 & 응용 가능 분야들\n",
    "    - 부분적 관측\n",
    "    - 협력\n",
    "    - multi-agent 학습\n",
    "        - 행위자팀이 각각의 관측에 기반하여 협력해야 하는 상황\n",
    "- 이러한 문제를 풀기 위해 넘어야 하는 것들:\n",
    "    - nonstationarity of learning\n",
    "        - Nonstationary process: a stochastic process whose unconditional joint probability distribution does change when shifted in time\n",
    "    - mult-agent credit assignment\n",
    "    - representing the value of joint actions\n",
    "- 벤치마킹은 주로 Arcade Learning Environment, MuJoCo 에서 많이 했으나, 이것은 single-agent deep RL 테스팅 용이었음\n",
    "- SMAC은 더 복잡한 mult-agent deep RL 테스팅을 위한 목적에서 제안됨\n",
    "    - partially observable environment\n",
    "    - dynamic conditions\n",
    "    - high-dimensional observation spaces\n",
    "- SC2 게임 엔진을 사용\n",
    "\n",
    "# SC2 (StarCraft 2)\n",
    "\n",
    "- 통상 SC2 게임은 1인 이상의 인간이 다른 인간(들)이나 AI를 상대로 진행함\n",
    "- 각 플레이어들은 자원을 모으고 건물을 짓고, 병력을 모음. \n",
    "- 승리 조건은 상대의 건물을 모두 부수는 것 (eliminate)\n",
    "- 통상적인 RTS (Real-Time Simulation) 게임처럼 SC2도 크게 두 부류의 게임플레이 요소가 존재함\n",
    "\n",
    "## Macromanagement (macro)\n",
    "\n",
    "- 고차원의 전략적 고려\n",
    "    - 경제 (자원확장), 자원 관리 (가스 중심? 미네랄 중심? 확장 시점 등)\n",
    "    - 테크트리 결정 (빠른 테크 후 후반 전략? 저테크 유닛으로 빠르게 공격? 등)\n",
    "    \n",
    "## Micromanagement (micro)\n",
    "\n",
    "- 개별 유닛의 전투 컨트롤\n",
    "    - 해병 산개 (대 맹독충)\n",
    "    - 분광기 컨트롤\n",
    "    - 추적자 점멸 컨트롤 등\n",
    "- 프로게이머들은 이러한 요소들을 직접 훈련하기도 함 (관련 챌린지가 SC2 아케이드 모드 중에 있음: \"스타크래프트 고수\" 유즈맵)\n",
    "\n",
    "## SC2 API\n",
    "\n",
    "- 블리자드사는 SC2 API를 통해 게임 조작을 외부적으로 가능하도록 했음\n",
    "- 딥마인드사는 PySC2를 개발하여 SC2 API를 RL 환경으로 기능할 수 있도록 만들어줌\n",
    "    - 알파스타 프로젝트는 PySC2를 통해 만들어줌\n",
    "    - 현재 알파스타는 PvP에서 프로 수준의 인간 플레이어를 상대로 승리를 거둔 단계임\n",
    "\n",
    "# SMAC\n",
    "\n",
    "- SMAC은 MARL 테스트베드 벤치마크를 제공하기 위해 고안됨\n",
    "- 이를 위해 오직 마이크로 컨트롤에만 초점을 맞춤\n",
    "- [SMAC video](https://youtu.be/VZ7zmQ_obZ0)에서는 실제로 위에 언급한 유즈맵과 비슷한 환경의 첼린지를 제시하고 있음 (해법도 유사함)\n",
    "    - focus fire (점사): 골고루 때리는 대신 일부 유닛에 화력을 집중하는 기술\n",
    "    - 포메이션 형성: 방어 타입 (중장갑/경장갑), 원거리/근접 공격여부, 지형활용 가능성 (공중, 언덕 공격 가능 여부, 시야) 등에 따라 형성\n",
    "- 위와 같은 플레이를 RL로 성취하는 것은 일종의 MARL의 효율성을 측정하기 위한 도구가 될 수 있다고 보고 있음\n",
    "\n",
    "## SMAC이 PySC2의 RL과 다른 점\n",
    "\n",
    "- PySC2는 SC2 풀게임을 대상으로 함\n",
    "    - 중앙집중적인 RL 에이전트가 인간 플레이어와 비슷한 환경에서 유사한 RGB 정보를 받아 태스크를 수행\n",
    "- SMAC은 협동적 MA들의 마이크로 챌린지임\n",
    "    - 여기에서는 각 agent가 단일 유닛을 조종함\n",
    "    \n",
    "| . | PySC2 | SMAC |\n",
    "| --- |:---:|:---:|\n",
    "| Setting | competitive | collaborative |\n",
    "|Control|player-level|unit-level|\n",
    "|Gameplay|macro & micro|micro|\n",
    "|Goal|master the full game of StarCraft II|benchmark cooperative MARL methods|\n",
    "|Observations|RGB pixels|feature vectors|\n",
    "|Replays available|yes|no|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAC: 구성요소\n",
    "\n",
    "- SMAC은 22개의 전투 시나리오\n",
    "- 상세 내용은 [SMAC 문서](https://github.com/oxwhirl/smac/blob/master/docs/smac.md)에 기술되어 있음\n",
    "\n",
    "|Name|Ally Units|Enemy Units|Type|\n",
    "|---|---|---|---|\n",
    "|3m|3 Marines|3 Marines|homogeneous & symmetric|\n",
    "|8m|8 Marines|8 Marines|homogeneous & symmetric|\n",
    "|25m|25 Marines|25 Marines|homogeneous & symmetric|\n",
    "|2s3z|2 Stalkers & 3 Zealots|2 Stalkers & 3 Zealots|heterogeneous & symmetric|\n",
    "|3s5z|3 Stalkers & 5 Zealots|3 Stalkers & 5 Zealots|heterogeneous & symmetric|\n",
    "|MMM|1 Medivac, 2 Marauders & 7 Marines|1 Medivac, 2 Marauders & 7 Marines|heterogeneous & symmetric|\n",
    "|5m_vs_6m|5 Marines|6 Marines|homogeneous & asymmetric|\n",
    "|8m_vs_9m|8 Marines|9 Marines|homogeneous & asymmetric|\n",
    "|10m_vs_11m|10 Marines|11 Marines|homogeneous & asymmetric|\n",
    "|27m_vs_30m|27 Marines|30 Marines|homogeneous & asymmetric|\n",
    "|3s5z_vs_3s6z|3 Stalkers & 5 Zealots|3 Stalkers & 6 Zealots|heterogeneous & asymmetric|\n",
    "|MMM2|1 Medivac, 2 Marauders & 7 Marines|1 Medivac, 3 Marauders & 8 Marines|heterogeneous & asymmetric|\n",
    "|2m_vs_1z|2 Marines|1 Zealot|micro-trick: alternating fire|\n",
    "|2s_vs_1sc|2 Stalkers|1 Spine Crawler|micro-trick: alternating fire|\n",
    "|3s_vs_3z|3 Stalkers|3 Zealots|micro-trick: kiting|\n",
    "|3s_vs_4z|3 Stalkers|4 Zealots|micro-trick: kiting|\n",
    "|3s_vs_5z|3 Stalkers|5 Zealots|micro-trick: kiting|\n",
    "|6h_vs_8z|6 Hydralisks|8 Zealots|micro-trick: focus fire|\n",
    "|corridor|6 Zealots|24 Zerglings|micro-trick: wall off|\n",
    "|bane_vs_bane|20 Zerglings & 4 Banelings|20 Zerglings & 4 Banelings|micro-trick: positioning|\n",
    "|so_many_banelings|7 Zealots|32 Banelings|micro-trick: positioning|\n",
    "|2c_vs_64zg|2 Colossi|64 Zerglings|micro-trick: positioning|\n",
    "\n",
    "## 상태와 관측\n",
    "\n",
    "- 모든 유닛들은 (관측선 제외) 동일 시야범위를 가지고 있음\n",
    "- 같은 팀의 유닛들은 시야를 공유함\n",
    "- 다른 유닛을 관찰하기 위해서는 자기 팀 유닛 중 최소한 하나가 시야 범위 안에 있어야 함\n",
    "\n",
    "### feature vector\n",
    "\n",
    "The feature vector observed by each agent contains the following attributes for both allied and enemy units within the sight range: \n",
    "\n",
    "- distance, \n",
    "- relative x, \n",
    "- relative y\n",
    "- shield\n",
    "- unit_type.\n",
    "\n",
    "### global state\n",
    "\n",
    "- The global state, which is only available to agents during centralised training, contains information about all units on the map. \n",
    "- Specifically, the state vector includes \n",
    "    - the coordinates of all agents relative to the centre of the map, together with unit features present in the observations. \n",
    "    - Additionally, the state stores the energy of Medivacs and cooldown of the rest of allied units, \n",
    "    - which represents the minimum delay between attacks. \n",
    "    - Finally, the last actions of all agents are attached to the central state.\n",
    "- All features, both in the state as well as in the observations of individual agents, are normalised by their maximum values. \n",
    "- The sight range is set to 9 for all agents.\n",
    "\n",
    "## Action Space\n",
    "\n",
    "- The discrete set of actions which agents are allowed to take consists of:\n",
    "    - move[direction] (four directions: north, south, east, or west._,\n",
    "    - attack[enemy_id], \n",
    "    - stop\n",
    "    - no-op. \n",
    "- Dead agents can only take no-op action while live agents cannot. \n",
    "- As healer units, Medivacs must use heal[agent_id] actions instead of attack[enemy_id]. \n",
    "- The maximum number of actions an agent can take ranges between 7 and 70, depending on the scenario.\n",
    "\n",
    "- To ensure decentralisation of the task, agents are restricted to use the attack[enemy_id] action only towards enemies in their shooting range. \n",
    "- This additionally constrains the unit's ability to use the built-in attack-move macro-actions on the enemies far away. \n",
    "- We set the shooting range equal to 6. \n",
    "- Having a larger sight than shooting range forces agents to make use of move commands before starting to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "- The overall goal is \n",
    "    - to have the highest win rate for each battle scenario. \n",
    "- We provide a corresponding option for sparse rewards, \n",
    "    - which will cause the environment to return only a reward of +1 for winning and -1 for losing an episode. \n",
    "- However, we also provide \n",
    "    - a default setting for a shaped reward signal calculated from the hit-point damage dealt and received by agents, \n",
    "    - some positive (negative) reward after having enemy (allied) units killed and/or \n",
    "    - a positive (negative) bonus for winning (losing) the battle. \n",
    "- The exact values and scales of this shaped reward can be configured using a range of flags, but \n",
    "- we strongly discourage disingenuous engineering of the reward function (e.g. tuning different reward functions for different scenarios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Settings\n",
    "\n",
    "- SMAC makes use of the StarCraft II Learning Environment (SC2LE) to communicate with the StarCraft II engine. \n",
    "    - SC2LE provides full control of the game by allowing to send commands and receive observations from the game. \n",
    "- However, SMAC is conceptually different from the RL environment of SC2LE. \n",
    "    - The goal of SC2LE is to learn to play the full game of StarCraft II.\n",
    "    - This is a competitive task where a centralised RL agent receives RGB pixels as input and performs both macro and micro with the player-level control similar to human players. SMAC, on the other hand, represents a set of cooperative multi-agent micro challenges where each learning agent controls a single military unit.\n",
    "\n",
    "- SMAC uses the raw API of SC2LE. \n",
    "    - Raw API observations do not have any graphical component and include information about the units on the map such as health, location coordinates, etc. \n",
    "    - The raw API also allows sending action commands to individual units using their unit IDs. \n",
    "    - This setting differs from how humans play the actual game, \n",
    "    - but is convenient for designing decentralised multi-agent learning tasks.\n",
    "\n",
    "- Since our micro-scenarios are shorter than actual StarCraft II games, \n",
    "- restarting the game after each episode presents a computational bottleneck. \n",
    "- To overcome this issue, we make use of the API's debug commands. \n",
    "    - Specifically, when all units of either army have been killed, \n",
    "    - we kill all remaining units by sending a debug action. \n",
    "    - Having no units left launches a trigger programmed with the StarCraft II Editor that re-spawns all units in their original location with full health, thereby restarting the scenario quickly and efficiently.\n",
    "\n",
    "- Furthermore, to encourage agents to explore interesting micro-strategies themselves, \n",
    "    - we limit the influence of the StarCraft AI on our agents. \n",
    "    - Specifically we disable the automatic unit attack against enemies that attack agents or that are located nearby. \n",
    "- To do so, we make use of new units created with the StarCraft II Editor that are exact copies of existing units with two attributes modified: \n",
    "    - Combat: Default Acquire Level is set to Passive (default Offensive) and \n",
    "    - Behaviour: Response is set to No Response (default Acquire). \n",
    "    - These fields are only modified for allied units; enemy units are unchanged.\n",
    "\n",
    "- The sight and shooting range values might differ from the built-in sight or range attribute of some StarCraft II units. \n",
    "- Our goal is not to master the original full StarCraft game, but rather to benchmark MARL methods for decentralised control.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
