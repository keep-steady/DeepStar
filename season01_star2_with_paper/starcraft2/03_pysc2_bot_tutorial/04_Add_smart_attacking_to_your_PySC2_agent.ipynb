{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주 문서 링크\n",
    "\n",
    "두 개의 문서로 되어 있음. 뒷문서는 앞문서의 예제 파일을 이어서 사용함\n",
    "\n",
    "- [Building a Smart PySC2 Agent](https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d)\n",
    "- [Add Smart Attacking to Your PySC2 Agent](https://itnext.io/add-smart-attacking-to-your-pysc2-agent-17fd5caad578)\n",
    "\n",
    "## 단계\n",
    "\n",
    "### Building a Smart PySC2 Agent\n",
    "\n",
    "1. Create the Agent\n",
    "2. Define the Actions\n",
    "3. Define the State\n",
    "4. Define the Rewards\n",
    "5. It’s Alive!\n",
    "\n",
    "### Add Smart Attacking to Your PySC2 Agent\n",
    "\n",
    "1. Set Up\n",
    "2. Alter the Attack Action\n",
    "3. Simplify the Actions\n",
    "4. Add Enemy Positions\n",
    "5. Simplify the State\n",
    "6. SCV Attack Cheat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Smart PySC2 Agent\n",
    "\n",
    "## Create the Agent\n",
    "\n",
    "- 주요 상수와 클래스 셋업. 테란용으로 기술되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분은 발제자 import 문제로 추가되어 있는 것임. 다른 분들은 실행할 필요 없음\n",
    "import sys\n",
    "sys.path.append(\"/Users/j/Documents/seminar/2019/DeepStar/venv/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id\n",
    "_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id\n",
    "_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id\n",
    "\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index\n",
    "_PLAYER_ID = features.SCREEN_FEATURES.player_id.index\n",
    "\n",
    "_PLAYER_SELF = 1\n",
    "\n",
    "_TERRAN_COMMANDCENTER = 18\n",
    "_TERRAN_SCV = 45 \n",
    "_TERRAN_SUPPLY_DEPOT = 19\n",
    "_TERRAN_BARRACKS = 21\n",
    "\n",
    "_NOT_QUEUED = [0]\n",
    "_QUEUED = [1]\n",
    "\n",
    "class SmartAgent_step1a(base_agent.BaseAgent):\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Q-Learning table class```를 추가. 여기에서 상태와 행동을 추적 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.ix[observation, :]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "        q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        \n",
    "        # update\n",
    "        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run:\n",
    "\n",
    "```bash\n",
    "python -m pysc2.bin.agent \\\n",
    "--map Simple64 \\\n",
    "--agent smart_agent.SmartAgent \\\n",
    "--agent_race T \\\n",
    "--max_agent_steps 0 \\\n",
    "--norender\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행동 정의하기\n",
    "\n",
    "- do nothing 도 유용한 행동임. 이 행동이 필요할 때도 있기 때문임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```step()```에 임의로 행동 결정하는 모듈을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartAgent_step2b(base_agent.BaseAgent):\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        smart_action = smart_actions[random.randrange(0, len(smart_actions) - 1)]\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FunctionCall(_NO_OP, [])\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                i = random.randint(0, len(unit_y) - 1)\n",
    "                target = [unit_x[i], unit_y[i]]\n",
    "                \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 0, int(unit_y.mean()), 20)\n",
    "                \n",
    "                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if _BUILD_BARRACKS in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 20, int(unit_y.mean()), 0)\n",
    "            \n",
    "                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                target = [int(unit_x.mean()), int(unit_y.mean())]\n",
    "        \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if _TRAIN_MARINE in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if _SELECT_ARMY in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if _ATTACK_MINIMAP in obs.observation[\"available_actions\"]:\n",
    "                if self.base_top_left:\n",
    "                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [39, 45]])\n",
    "            \n",
    "                return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [21, 24]])\n",
    "            \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning Tb 추가\n",
    "```python\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태 정의\n",
    "\n",
    "- QL에서 사용할 상태 이름을 정의\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartAgent_step3a(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "    \n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ##################\n",
    "        unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "\n",
    "        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()\n",
    "        supply_depot_count = 1 if depot_y.any() else 0\n",
    "\n",
    "        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "        barracks_count = 1 if barracks_y.any() else 0\n",
    "            \n",
    "        supply_limit = obs.observation['player'][4]\n",
    "        army_supply = obs.observation['player'][5]\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        #smart_action = smart_actions[random.randrange(0, len(smart_actions) - 1)]\n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        ####################\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FunctionCall(_NO_OP, [])\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                i = random.randint(0, len(unit_y) - 1)\n",
    "                target = [unit_x[i], unit_y[i]]\n",
    "                \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 0, int(unit_y.mean()), 20)\n",
    "                \n",
    "                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if _BUILD_BARRACKS in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 20, int(unit_y.mean()), 0)\n",
    "            \n",
    "                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                target = [int(unit_x.mean()), int(unit_y.mean())]\n",
    "        \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if _TRAIN_MARINE in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if _SELECT_ARMY in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if _ATTACK_MINIMAP in obs.observation[\"available_actions\"]:\n",
    "                if self.base_top_left:\n",
    "                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [39, 45]])\n",
    "            \n",
    "                return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [21, 24]])\n",
    "            \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보상 정의\n",
    "\n",
    "- global 상수부에 아래 줄 추가\n",
    "- QL에서 스코어를 담당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스코어 반영을 위해 ```step()``` 에 유지하기 위한 단조증가하는 스코어를 유지\n",
    "```python\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스코어 유지부\n",
    "\n",
    "```python\n",
    "\n",
    "        killed_unit_score = obs.observation['score_cumulative'][5]\n",
    "        killed_building_score = obs.observation['score_cumulative'][6]\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        reward = 0\n",
    "            \n",
    "        if killed_unit_score > self.previous_killed_unit_score:\n",
    "            reward += KILL_UNIT_REWARD\n",
    "                \n",
    "        if killed_building_score > self.previous_killed_building_score:\n",
    "            reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update score\n",
    "\n",
    "```python\n",
    "\n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 처리\n",
    "\n",
    "- 선택한 액션의 결과를 학습할 수 있게 하기 위해 이전 상태와 행동을 정의\n",
    "\n",
    "```python\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QL에 지금까지 정의한 스코어 관련부들을 모두 추가\n",
    "\n",
    "```python\n",
    "\n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 코드는 smart_agent.py임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id\n",
    "_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id\n",
    "_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id\n",
    "\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index\n",
    "_PLAYER_ID = features.SCREEN_FEATURES.player_id.index\n",
    "\n",
    "_PLAYER_SELF = 1\n",
    "\n",
    "_TERRAN_COMMANDCENTER = 18\n",
    "_TERRAN_SCV = 45 \n",
    "_TERRAN_SUPPLY_DEPOT = 19\n",
    "_TERRAN_BARRACKS = 21\n",
    "\n",
    "_NOT_QUEUED = [0]\n",
    "_QUEUED = [1]\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5\n",
    "\n",
    "# Stolen from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.ix[observation, :]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "        q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        \n",
    "        # update\n",
    "        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))\n",
    "\n",
    "class SmartAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "\n",
    "        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()\n",
    "        supply_depot_count = supply_depot_count = 1 if depot_y.any() else 0\n",
    "\n",
    "        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "        barracks_count = 1 if barracks_y.any() else 0\n",
    "            \n",
    "        supply_limit = obs.observation['player'][4]\n",
    "        army_supply = obs.observation['player'][5]\n",
    "        \n",
    "        killed_unit_score = obs.observation['score_cumulative'][5]\n",
    "        killed_building_score = obs.observation['score_cumulative'][6]\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FunctionCall(_NO_OP, [])\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                i = random.randint(0, len(unit_y) - 1)\n",
    "                target = [unit_x[i], unit_y[i]]\n",
    "                \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 0, int(unit_y.mean()), 20)\n",
    "                \n",
    "                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if _BUILD_BARRACKS in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 20, int(unit_y.mean()), 0)\n",
    "            \n",
    "                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                target = [int(unit_x.mean()), int(unit_y.mean())]\n",
    "        \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if _TRAIN_MARINE in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if _SELECT_ARMY in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if _ATTACK_MINIMAP in obs.observation[\"available_actions\"]:\n",
    "                if self.base_top_left:\n",
    "                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [39, 45]])\n",
    "            \n",
    "                return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [21, 24]])\n",
    "        \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m pysc2.bin.agent \\\n",
    "--map Simple64 \\\n",
    "--agent smart_agent.SmartAgent \\\n",
    "--agent_race terran \\\n",
    "--max_agent_steps 0 \\\n",
    "--norender\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
